<!DOCTYPE html>
<html>
<head>
<title>report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="mpi%E5%AE%9E%E7%8E%B0parallel-sorting-by-regular-sampling-%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A">MPI实现Parallel Sorting by Regular Sampling 实验报告</h1>
<p>姓名：杨子旭  学号：21307130105</p>
<h2 id="1-%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84">1. 实验目的</h2>
<ul>
<li>用Message Passing Interface实现PSRS算法</li>
<li>测量该算法在不同数组规模和进程数量下的用时</li>
<li>测量该算法在不同数组规模和进程数量下的加速比以衡量其性能</li>
<li>测量PSRS算法的四个阶段的用时情况</li>
</ul>
<h2 id="2-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF">2. 实现思路</h2>
<h3 id="21-%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92">2.1 参数传递</h3>
<p>为了便于测量该算法在不同数组规模和进程数量下的用时，以及测量PSRS算法的四个阶段的用时情况，我想让我的程序能够从命令行读取数组大小、是否分阶段输出计时这两个参数。进程数量可以由<code>MPI_Comm_size</code>获取，不需要额外的参数传递。</p>
<pre class="hljs"><code><div>      <span class="hljs-keyword">static</span> <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> *<span class="hljs-keyword">const</span> OPT_STR = <span class="hljs-string">"l:o"</span>;
        <span class="hljs-keyword">static</span> <span class="hljs-keyword">const</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">option</span> <span class="hljs-title">OPTS</span>[] = {</span>
                {<span class="hljs-string">"length"</span>,   required_argument, <span class="hljs-literal">NULL</span>, <span class="hljs-string">'l'</span>}, <span class="hljs-comment">//数组长度</span>
                {<span class="hljs-string">"outphase"</span>,   no_argument, <span class="hljs-literal">NULL</span>, <span class="hljs-string">'o'</span>}};    <span class="hljs-comment">//是否输出各个阶段的用时</span>
</div></code></pre>
<p>为了防止多个进程同时获取命令行参数引起混乱，我只让主进程获取参数，然后将整理好的参数广播到其余进程：</p>
<pre class="hljs"><code><div><span class="hljs-comment">/* arg 结构体中成员的值从根进程（排名为0的进程）
广播到 MPI 通信组中的所有其他进程。
这样，所有的进程都能获得相同的arg结构体中的值*/</span>
<span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">argument_bcast</span><span class="hljs-params">(struct arguments *arg)</span>
</span>{
        MPI_Barrier(MPI_COMM_WORLD);
        MPI_Bcast(&amp;(arg-&gt;length), <span class="hljs-number">1</span>, MPI_INT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);
        MPI_Barrier(MPI_COMM_WORLD);
        MPI_Bcast(&amp;(arg-&gt;procnum), <span class="hljs-number">1</span>, MPI_INT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);
        MPI_Barrier(MPI_COMM_WORLD);
        MPI_Bcast(&amp;(arg-&gt;outphase), <span class="hljs-number">1</span>, MPI_INT, <span class="hljs-number">0</span>, MPI_COMM_WORLD);
        MPI_Barrier(MPI_COMM_WORLD);
}
</div></code></pre>
<p>此后，由于进程间内存不共享，因此每个进程都有必要“记住”自己的和公有的全部参数。</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">procargs</span> {</span> <span class="hljs-comment">// 每个进程的参数</span>
        <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">int</span> root; <span class="hljs-comment">// 是否是根进程</span>
        <span class="hljs-keyword">int</span> id; <span class="hljs-comment">// 进程的ID</span>
        <span class="hljs-keyword">int</span> procnum; <span class="hljs-comment">// 本次运行的进程总数</span>
        <span class="hljs-keyword">long</span> *head; <span class="hljs-comment">// 进程自己分到的数组的首地址</span>
        <span class="hljs-keyword">int</span> size; <span class="hljs-comment">//进程自己分到的数组段大小</span>
        <span class="hljs-keyword">int</span> max_sample_size; <span class="hljs-comment">// 最大采样大小</span>
        <span class="hljs-keyword">int</span> total_size;<span class="hljs-comment">// 全局排序任务的数组大小</span>
};
</div></code></pre>
<h3 id="22-psrs%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0">2.2 PSRS算法实现</h3>
<p>下图简明生动地描述了Parallel Sorting by Regular Sampling的全过程，主要分为四个阶段：</p>
<ol>
<li>主进程均匀分割原数组为p份，发送给其他的进程，所有进程对分到的数组进行局部排序，按间隔$\frac{n}{p^2}$正则采样，将样本发送给主进程。</li>
<li>主进程对收集到的样本进行排序，按间隔p对样本采样选取主元，然后把主元广播到各个进程内。</li>
<li>所有进程根据主元来划分自己的数组。</li>
<li>进程间相互交换划分后的数组块，然后各自进行归并排序。最后主进程依次收集到所有进程的排序结果，连在一起就是排序号的数组。
<img src="image.png" alt="Alt text"></li>
</ol>
<h4 id="221-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E8%AF%B4%E6%98%8E">2.2.1 数据结构设计与说明</h4>
<p>由上面的流程图，PSRS算法需要用到的数据结构是<strong>数组</strong>。为了提升算法性能，我没有使用C++的 STL库<code>vector</code>来作为数组的数据结构，虽然<code>vector</code>可以自动管理内存，但是由于它每次超出已分配内存大小，都会先新分配一块2倍于原有大小的新空间，再把原有的数组复制过去，这导致了巨大的时间和空间开销的增长。所以我仅只用了<code>array</code>,每次存储时会根据大小手动用<code>calloc</code>函数分配内存，每次复制时用<code>memcpy</code>函数直接复制整块内存。为了便于获取array中元素的数量，我把元素数量和array首地址合并为一个结构体：<code>arr_segment</code>:</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">arr_segment</span> {</span> <span class="hljs-comment">//一个数组段</span>
        <span class="hljs-keyword">long</span> *head; <span class="hljs-comment">//首元素的地址</span>
        <span class="hljs-keyword">int</span> size; <span class="hljs-comment">//数组段的长度</span>
};
</div></code></pre>
<p>此外，由于每个进程会管理一个自己的数组序列，所以我为每个进程提供了一个结构体<code>segment_block</code>：</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">segment_block</span> {</span> <span class="hljs-comment">//每个进程管理自己的区块，区块内可能含有多个数组段</span>
        <span class="hljs-keyword">bool</span> clean; 
        <span class="hljs-keyword">int</span> size; <span class="hljs-comment">//有多少个数组段</span>
        <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">arr_segment</span> <span class="hljs-title">part</span>[];</span> <span class="hljs-comment">//数组段的列表</span>
};
</div></code></pre>
<h4 id="222-%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5--%E5%9D%87%E5%8C%80%E5%88%92%E5%88%86%E5%B1%80%E9%83%A8%E6%8E%92%E5%BA%8F%E6%AD%A3%E5%88%99%E9%87%87%E6%A0%B7">2.2.2 第一阶段 —— 均匀划分、局部排序、正则采样</h4>
<ol>
<li>均匀划分
在<code>local_scatter</code>函数中，首先为每个进程分配了存储局部数据的空间 <code>arg-&gt;head</code>。接着，根进程通过 MPI 的 <code>MPI_Bcast</code> 函数向其他进程广播了每个进程应该接收的局部数据的大小 <code>arg-&gt;max_sample_size</code>。接下来，使用 MPI 的 <code>MPI_Scatter</code> 函数将原始数据 array 均匀划分给每个进程，每个进程接收到的数据存储在 <code>arg-&gt;head</code> 中。</li>
</ol>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span>
<span class="hljs-title">local_scatter</span><span class="hljs-params">(<span class="hljs-keyword">long</span> <span class="hljs-built_in">array</span>[], struct procargs *<span class="hljs-keyword">const</span> arg)</span>
</span>{
    <span class="hljs-comment">// 为每个进程分配存储局部数据的空间</span>
    arg-&gt;head = (<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(arg-&gt;size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>)); 
    MPI_Barrier(MPI_COMM_WORLD); 
    <span class="hljs-comment">// 由根进程广播每个进程应该接收的局部数据的大小</span>
    MPI_Bcast(&amp;(arg-&gt;max_sample_size), <span class="hljs-number">1</span>, MPI_INT, arg-&gt;procnum - <span class="hljs-number">1</span>, MPI_COMM_WORLD); 
    MPI_Barrier(MPI_COMM_WORLD);
    <span class="hljs-comment">// 使用 MPI_Scatter 函数将原始数据均匀划分给每个进程</span>
    MPI_Scatter(<span class="hljs-built_in">array</span>, arg-&gt;size, MPI_LONG, arg-&gt;head, arg-&gt;size, MPI_LONG, <span class="hljs-number">0</span>, MPI_COMM_WORLD);
}
</div></code></pre>
<ol start="2">
<li>局部排序和正则采样
<code>local_sort</code>函数负责在每个进程内对接收到的局部数据进行局部排序。首先，定义了一个采样步长 <code>window</code>，用于在排序后选择采样的数据点。然后，使用快速排序算法<code>qsort</code> 对局部数据 <code>arg-&gt;head</code> 进行排序。接着，通过链表 <code>local_sample_list</code> 存储采样结果，避免频繁的内存分配。最后，将链表中的采样点转移到 <code>local_samples</code> 结构中，其中包括采样点的大小和头指针。</li>
</ol>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span>
<span class="hljs-title">local_sort</span><span class="hljs-params">(struct arr_segment *<span class="hljs-keyword">const</span> local_samples, <span class="hljs-keyword">const</span> struct procargs *<span class="hljs-keyword">const</span> arg)</span>
</span>{
    <span class="hljs-comment">// 采样步长，用于在排序后选择采样的数据点</span>
    <span class="hljs-keyword">int</span> window = arg-&gt;total_size / (arg-&gt;procnum * arg-&gt;procnum);
    List local_sample_list;
    <span class="hljs-built_in">memset</span>(local_samples, <span class="hljs-number">0</span>, <span class="hljs-keyword">sizeof</span>(struct arr_segment));
    <span class="hljs-comment">// 在每个进程内使用快速排序进行局部排序</span>
    qsort(arg-&gt;head, arg-&gt;size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>), long_compare);
    <span class="hljs-comment">// 使用链表存储采样结果</span>
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> idx = <span class="hljs-number">0</span>, picked = <span class="hljs-number">0</span>; idx &lt; arg-&gt;size &amp;&amp; picked &lt; arg-&gt;max_sample_size; idx += window, ++picked)
    {   <span class="hljs-comment">// 将采样点添加到链表中</span>
        local_sample_list.add(arg-&gt;head[idx]);
    }
    <span class="hljs-comment">// 将采样结果转移到 local_samples 结构中</span>
    local_samples-&gt;size = local_sample_list.getSize();
    local_samples-&gt;head = (<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(local_samples-&gt;size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
    <span class="hljs-comment">// 将链表中的数据拷贝到 local_samples</span>
    <span class="hljs-keyword">long</span> *<span class="hljs-built_in">array</span> = <span class="hljs-keyword">new</span> <span class="hljs-keyword">long</span>[local_samples-&gt;size];
    local_sample_list.copyTo(<span class="hljs-built_in">array</span>);
    <span class="hljs-built_in">memcpy</span>(local_samples-&gt;head, <span class="hljs-built_in">array</span>, local_samples-&gt;size * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
    <span class="hljs-keyword">delete</span>[] <span class="hljs-built_in">array</span>;
    MPI_Barrier(MPI_COMM_WORLD);
}
</div></code></pre>
<h4 id="223-%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5--%E6%94%B6%E9%9B%86%E5%B1%80%E9%83%A8%E6%A0%B7%E6%9C%AC%E6%8E%92%E5%BA%8F%E5%90%8E%E9%80%89%E5%8F%96%E4%B8%BB%E5%85%83%E5%B9%BF%E6%92%AD%E4%B8%BB%E5%85%83">2.2.3 第二阶段 —— 收集局部样本、排序后选取主元、广播主元</h4>
<ol>
<li>收集局部样本
<code>pivots_bcast</code> 函数通过 MPI 的 <code>MPI_Reduce</code> 函数，将每个进程的局部采样点数目 <code>local_samples-&gt;size</code> 汇总到根进程（rank 0）的 <code>total_samples.size</code> 中。<pre class="hljs"><code><div><span class="hljs-comment">// 使用MPI_Reduce将每个进程的局部采样点数目汇总到根进程的total_samples.size中</span>
MPI_Reduce(&amp;(local_samples-&gt;size),
           &amp;(total_samples.size),
           <span class="hljs-number">1</span>,
           MPI_INT,
           MPI_SUM,
           <span class="hljs-number">0</span>,
           MPI_COMM_WORLD);
</div></code></pre>
在根进程中，为 <code>total_samples.head</code> 分配足够的空间，并通过 MPI 的 <code>MPI_Gather</code> 函数，收集所有进程的局部采样点数据到 <code>total_samples.head</code> 中。<pre class="hljs"><code><div><span class="hljs-comment">// 使用MPI_Gather收集所有进程的局部采样点数据到total_samples.head中</span>
MPI_Gather(local_samples-&gt;head,
           local_samples-&gt;size,
           MPI_LONG,
           total_samples.head,
           local_samples-&gt;size,
           MPI_LONG,
           <span class="hljs-number">0</span>,
           MPI_COMM_WORLD);
</div></code></pre>
</li>
<li>排序样本点并选取主元
接着，根据采样数据，根进程进行全局排序，选择每个进程所需的主元，即分割点。通过均匀选择的方式，选取了<code>arg-&gt;procnum</code> - 1 个主元，这保证了后续划分的块尽可能均匀。<pre class="hljs"><code><div>    <span class="hljs-comment">// 对total_samples.head进行全局排序</span>
    qsort(total_samples.head,
          total_samples.size,
          <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>),
          long_compare);
    <span class="hljs-comment">// 从排序后的采样点中选取每个进程所需的主元</span>
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = pivot_step, count = <span class="hljs-number">0</span>;
         i &lt; total_samples.size &amp;&amp; count &lt; arg-&gt;procnum - <span class="hljs-number">1</span>;
         i += arg-&gt;procnum, ++count) {
        <span class="hljs-comment">// 将选取的主元添加到pivot_list中</span>
        <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> &gt; pivot_list.add(total_samples.head[i])) {
            MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);
        }
    }
</div></code></pre>
</li>
<li>广播主元
最后，通过 MPI 的 <code>MPI_Bcast</code> 函数将选取的主元广播给所有进程，每个进程根据收到的主元信息分配 pivots-&gt;head 空间，存储自己所需的主元数据。<pre class="hljs"><code><div><span class="hljs-comment">// 使用MPI_Bcast广播pivots-&gt;size到所有进程</span>
MPI_Bcast(&amp;(pivots-&gt;size),
          <span class="hljs-number">1</span>,
          MPI_INT,
          <span class="hljs-number">0</span>,
          MPI_COMM_WORLD);
<span class="hljs-comment">// 如果不是根进程，为pivots-&gt;head分配空间</span>
<span class="hljs-keyword">if</span> (arg-&gt;root == <span class="hljs-literal">false</span>) {
    pivots-&gt;head = (<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(pivots-&gt;size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));}

<span class="hljs-comment">// 使用MPI_Bcast广播pivots-&gt;head到所有进程</span>
MPI_Bcast(pivots-&gt;head,
          pivots-&gt;size,
          MPI_LONG,
          <span class="hljs-number">0</span>,
          MPI_COMM_WORLD);
</div></code></pre>
</li>
</ol>
<p>这个阶段的设计是为了确保每个进程都能获得相同的主元，并为下一阶段的划分做好准备。主元的均匀选择有助于保持后续划分的负载平衡。</p>
<h4 id="224-%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5--%E6%89%80%E6%9C%89%E8%BF%9B%E7%A8%8B%E6%A0%B9%E6%8D%AE%E4%B8%BB%E5%85%83%E6%9D%A5%E5%88%92%E5%88%86%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E7%BB%84%E4%BA%A4%E6%8D%A2%E5%88%92%E5%88%86%E5%9D%97">2.2.4 第三阶段 —— 所有进程根据主元来划分自己的数组、交换划分块</h4>
<ol>
<li>用主元来划分自己的数组
<code>arr_segment_form</code>函数让所有进程根据主元，用二分查找的方法找到“分割点”，然后形成自己的数组块。这里比较难考虑的是临界情况，比如两个分割点在同一个位置的情况。此时需要兼顾三种情况：此位置在首、中、尾。其中“首”和“中”的处理方法是将size置0，head(划分块中一个划分单元的首地址)保持在上一个划分单元的尾部。尾的处理方式较为特殊，因为如果此时继续用<code>binary search</code>，会导致数组访问越界，引发<code>segmentation fault</code>。因此我设置了侦察划分块是否已经满足数组大小的逻辑，一旦满足，就立刻退出，不再进行后面的查找。然后将退出点后面的划分单元的size全部置0.</li>
</ol>
<pre class="hljs"><code><div>    <span class="hljs-comment">// 初始划分块的头指针为数组的头指针</span>
    blk-&gt;part[part_idx].head = arg-&gt;head;
    <span class="hljs-comment">// 遍历选取的主元</span>
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> pivot_idx = <span class="hljs-number">0</span>; pivot_idx &lt; pivots-&gt;size; ++pivot_idx) {
        pivot = pivots-&gt;head[pivot_idx];
        <span class="hljs-comment">// 通过二分查找获取当前主元在当前划分块中的位置</span>
        <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> &gt; binary_search(&amp;sub_idx,
                              pivot,
                              blk-&gt;part[part_idx].head,
                              arg-&gt;size - prev_part_size)) {
            MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);
        }
        <span class="hljs-comment">// 设置当前划分块的大小</span>
        blk-&gt;part[part_idx].size = sub_idx;
        prev_part_size += sub_idx;
        <span class="hljs-comment">// 如果当前划分块已满足数组大小，标记终止，并记录终止位置</span>
        <span class="hljs-keyword">if</span>(arg-&gt;size - prev_part_size == <span class="hljs-number">0</span>) {
            <span class="hljs-built_in">terminate</span> = <span class="hljs-number">1</span>;
            terminate_from = part_idx + <span class="hljs-number">1</span>;
            <span class="hljs-keyword">break</span>;
        }
        <span class="hljs-comment">// 设置下一个划分块的头指针</span>
        blk-&gt;part[part_idx + <span class="hljs-number">1</span>].head = blk-&gt;part[part_idx].head +
                                        blk-&gt;part[part_idx].size;
        ++part_idx;
    }
    <span class="hljs-comment">// 如果终止标记为真，将后续划分块的头指针和大小置空</span>
    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">terminate</span>) {
        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = terminate_from; i &lt;= pivots-&gt;size; i++) {
            blk-&gt;part[i].head = <span class="hljs-literal">NULL</span>;
            blk-&gt;part[i].size = <span class="hljs-number">0</span>;
        }
    } <span class="hljs-keyword">else</span> {
        <span class="hljs-comment">// 如果没有终止，设置最后一个划分块的大小</span>
        blk-&gt;part[part_idx].size = arg-&gt;size - prev_part_size;
    }
    <span class="hljs-built_in">free</span>(pivots-&gt;head);
    MPI_Barrier(MPI_COMM_WORLD);
</div></code></pre>
<ol start="2">
<li>交换划分块
这里的发送和接收的逻辑都比较复杂。目标是让第k个进程收集其余进程的第k段划分块。
方法是先遍历所有的进程，依次定义它们成为发送者<pre class="hljs"><code><div>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; arg-&gt;procnum; ++i) {
             arr_segment_send(blk_copy, blk, i, &amp;part_idx, arg);
     }
</div></code></pre>
接下来在<code>arr_segment_send</code>函数中，如果识别出是sender自己，就自己拷贝自己的第k段（k是进程自己的序号）。需要额外验证那一段是不是空的，如果第k段的size为0，则不拷贝。<pre class="hljs"><code><div><span class="hljs-comment">/*如果是sender自己*/</span>
    <span class="hljs-keyword">if</span> (sid == arg-&gt;id) {
            blk_copy-&gt;part[*pindex].size = blk-&gt;part[arg-&gt;id].size;
            <span class="hljs-keyword">if</span>(blk_copy-&gt;part[*pindex].size&gt;<span class="hljs-number">0</span>){
            blk_copy-&gt;part[*pindex].head = (<span class="hljs-keyword">long</span> *)\
                                           <span class="hljs-built_in">calloc</span>(blk-&gt;part[arg-&gt;id].size,
                                                  <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));

            <span class="hljs-built_in">memcpy</span>(blk_copy-&gt;part[*pindex].head, blk-&gt;part[arg-&gt;id].head,
                   blk-&gt;part[arg-&gt;id].size * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));}
            <span class="hljs-keyword">else</span> blk_copy-&gt;part[*pindex].head = <span class="hljs-literal">NULL</span>;
            ++*pindex;
    }
</div></code></pre>
然后遍历自己的划分块，假设现在遍历到第j块。如果j与k（k是进程自己的序号）相等，就什么也不做。
如果j与k不等，且自己是当前的sender，就依次发送自己的第j块给第j个进程；<pre class="hljs"><code><div><span class="hljs-keyword">if</span> (sid == arg-&gt;id) {       <span class="hljs-comment">//先发送大小信息</span>
                            MPI_Ssend(&amp;(blk-&gt;part[j].size),
                                      <span class="hljs-number">1</span>,
                                      MPI_INT,
                                      j,
                                      <span class="hljs-number">0</span>,
                                      MPI_COMM_WORLD);
                            <span class="hljs-keyword">if</span>(blk-&gt;part[j].size!=<span class="hljs-number">0</span>){
                            <span class="hljs-comment">//把自己的第j块发送给第j个进程</span>
                            MPI_Ssend(blk-&gt;part[j].head,
                                      blk-&gt;part[j].size,
                                      MPI_LONG,
                                      j,
                                      <span class="hljs-number">0</span>,
                                      MPI_COMM_WORLD);}
                    }
</div></code></pre>
自己不是当前的sender，且当前遍历到的划分快正是自己想要的，就试图接收当前的sender（第j个进程）发送来的第k块。一共经过$p^2$轮循环，就可以实现让第k个进程都收集其余进程的第k段划分块。<pre class="hljs"><code><div><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (j == arg-&gt;id) {    <span class="hljs-comment">//先接受大小信息</span>
                            MPI_Recv(&amp;(blk_copy-&gt;part[*pindex].size),
                                     <span class="hljs-number">1</span>,
                                     MPI_INT,
                                     sid,
                                     MPI_ANY_TAG,
                                     MPI_COMM_WORLD,
                                     &amp;recv_status);
                            mpi_recv_check(&amp;recv_status,
                                           MPI_INT,
                                           <span class="hljs-number">1</span>);
                            <span class="hljs-keyword">if</span>(blk_copy-&gt;part[*pindex].size&gt;<span class="hljs-number">0</span>){ 
                            blk_copy-&gt;part[*pindex].head =(<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(\
                                            blk_copy-&gt;part[*pindex].size,
                                            <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>)); <span class="hljs-comment">//为要接受的数组分配空间</span>
                            <span class="hljs-keyword">if</span> (<span class="hljs-literal">NULL</span> == blk_copy-&gt;part[*pindex].head) {
                                    MPI_Abort(MPI_COMM_WORLD,
                                              EXIT_FAILURE);
                            }
                            <span class="hljs-comment">//接受第j个进程发来的第k块</span>
                            MPI_Recv(blk_copy-&gt;part[*pindex].head,
                                     blk_copy-&gt;part[*pindex].size,
                                     MPI_LONG,
                                     sid,
                                     MPI_ANY_TAG,
                                     MPI_COMM_WORLD,
                                     &amp;recv_status);
                            <span class="hljs-comment">//核实是否顺利接收</span>
                            mpi_recv_check(&amp;recv_status,
                                           MPI_LONG,
                                           blk_copy-&gt;part[*pindex].size);}
</div></code></pre>
</li>
</ol>
<h4 id="225-%E7%AC%AC%E5%9B%9B%E9%98%B6%E6%AE%B5--%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F">2.2.5 第四阶段 —— 归并排序</h4>
<ol>
<li>归并排序
所有进程对自己的数组块进行多项归并排序。这里可以转化为多个二项归并排序：<pre class="hljs"><code><div>    running_result = blk_copy-&gt;part[<span class="hljs-number">0</span>]; <span class="hljs-comment">//初始结果</span>
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; arg-&gt;procnum; ++i) {
            merge_dump.size = running_result.size + blk_copy-&gt;part[i].size;
            <span class="hljs-keyword">if</span>(blk_copy-&gt;part[i].size&gt;<span class="hljs-number">0</span>){ <span class="hljs-comment">//对于size=0的块，直接忽略</span>
            merge_dump.head = (<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(merge_dump.size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
            <span class="hljs-comment">//归并</span>
            array_merge(merge_dump.head,
                        running_result.head,
                        running_result.size,
                        blk_copy-&gt;part[i].head,
                        blk_copy-&gt;part[i].size);
            running_result = merge_dump;}
    }
</div></code></pre>
然后把自己的结果存在一个数组列表的第k项中，k是自己的进程序号。<pre class="hljs"><code><div>
    temp_result[arg-&gt;id].size = running_result.size;
    temp_result[arg-&gt;id].head = (<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(running_result.size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
    <span class="hljs-built_in">memcpy</span>(temp_result[arg-&gt;id].head, running_result.head,running_result.size*<span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
</div></code></pre>
最后所有进程把自己的第k项发送给主进程的相同位置<pre class="hljs"><code><div>MPI_Recv(temp_result[i].head,
                             merged_size,
                             MPI_LONG,
                             i,
                             MPI_ANY_TAG,
                             MPI_COMM_WORLD,
                             &amp;recv_status);
</div></code></pre>
主进程按顺序读取这个列表就可以得到排序的结果了<pre class="hljs"><code><div><span class="hljs-keyword">if</span>(arg-&gt;root){
           <span class="hljs-keyword">int</span> lastsize = <span class="hljs-number">0</span>;
            <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; arg-&gt;procnum; i++){
                    <span class="hljs-built_in">memcpy</span>((result-&gt;head + lastsize), temp_result[i].head, (temp_result[i].size) * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
                    lastsize += temp_result[i].size;
                    
            }
    }
</div></code></pre>
</li>
</ol>
<h2 id="3-%E6%B5%8B%E8%AF%95%E8%BF%87%E7%A8%8B">3. 测试过程</h2>
<h3 id="31-%E6%B5%8B%E9%87%8F%E6%97%B6%E9%97%B4%E7%9A%84%E6%96%B9%E6%B3%95">3.1 测量时间的方法</h3>
<p>在 <code>sort.cpp</code>的<code>parallel_sort</code> 函数中,对同一个实验进行了10次，取平均时间。</p>
<pre class="hljs"><code><div>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">10</span>; ++i) { <span class="hljs-comment">//运行十次，对时间取平均数</span>
                psort_wrapper(per_phase_sort_time, arg);           
                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; PHASE_COUNT; ++j) {
                        avg_per_phase_sort_time[j] += (per_phase_sort_time[j]/<span class="hljs-number">10</span>); <span class="hljs-comment">//每个phase的运行时间</span>
                        total_sort_time += (per_phase_sort_time[j]/<span class="hljs-number">10</span>); <span class="hljs-comment">//总运行时间</span>
                }       
                MPI_Barrier(MPI_COMM_WORLD); 
        }
</div></code></pre>
<p>为了测量每个phase的用时，我在两个phase之间让主进程来计时。</p>
<pre class="hljs"><code><div>        <span class="hljs-keyword">if</span> (arg-&gt;root) {
                timing_stop((elapsed[PHASE1]), start);
                timing_reset(start);
                timing_start(start);
        }
</div></code></pre>
<p>如果用户指定分phase输出，就输出四个数。如果输出整个的排序用时，就求和再输出。</p>
<pre class="hljs"><code><div>        <span class="hljs-keyword">if</span> (arg-&gt;outphase) { <span class="hljs-comment">//如果要分段输出</span>
                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; PHASE_COUNT; ++j) {
                                psort_stats[j] += per_phase_sort_time[j];
                        }
        } <span class="hljs-keyword">else</span> { <span class="hljs-comment">//如果要输出整块的时间</span>
                psort_stats[<span class="hljs-number">0</span>] = total_sort_time;
        }
</div></code></pre>
<h3 id="32-%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E9%87%8F%E6%96%B9%E6%B3%95">3.2 自动化测量方法</h3>
<p>为了测试并行排序算法在不同的数据规模、不同的线程数下的表现，相比串行排序的加速比等等，我编写了一个python程序<code>run.py</code>，它能够用不同的参数组合运行可执行文件，然后将得到的数据（时间）整合存储到一个csv文件，以便后续数据可视化的处理。
参数设定：</p>
<pre class="hljs"><code><div>    program_path = <span class="hljs-string">"./executable/psrs"</span>
    mpi_run = <span class="hljs-string">"mpirun -np {procnum} "</span>
    len_flag = <span class="hljs-string">" -l {length}"</span>
    program = mpi_run + program_path + len_flag

    procnum_range = tuple(<span class="hljs-number">2</span>**e <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>))
    size_range = tuple(<span class="hljs-number">2</span>**e <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> range(<span class="hljs-number">16</span>, <span class="hljs-number">20</span>, <span class="hljs-number">1</span>))
</div></code></pre>
<h3 id="33-%E6%95%B0%E7%BB%84%E7%94%9F%E6%88%90%E5%92%8C%E6%8E%92%E5%BA%8F%E6%AD%A3%E7%A1%AE%E6%80%A7%E9%AA%8C%E8%AF%81">3.3 数组生成和排序正确性验证</h3>
<p>为了避免特定的数组元素排列导致排序时间受影响，我让每次运行并行算法函数都随机生成一个数组。并且在排序完成后验证排序后的数组是不是顺序的。数组由时间种子随机生成，验证排序正确性由主进程在最后完成：</p>
<pre class="hljs"><code><div>              <span class="hljs-comment">/*验证排序结果是否正确*/</span>
                <span class="hljs-keyword">long</span> *cmp = (<span class="hljs-keyword">long</span> *)<span class="hljs-built_in">calloc</span>(arg-&gt;total_size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
                <span class="hljs-built_in">memcpy</span>(cmp, result.head, arg-&gt;total_size * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>));
                qsort(cmp, arg-&gt;total_size, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>), long_compare);
                <span class="hljs-keyword">if</span>(<span class="hljs-built_in">memcmp</span>(cmp, result.head, arg-&gt;total_size * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">long</span>))!=<span class="hljs-number">0</span>){
                       <span class="hljs-built_in">puts</span>(<span class="hljs-string">"The Result is Wrong!"</span>);<span class="hljs-comment">//如果不正确，会中止程序</span>
                       MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE); 
                }
</div></code></pre>
<h3 id="34-%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E6%96%B9%E6%B3%95%E5%92%8C%E7%BB%93%E6%9E%9C">3.4 测试程序运行方法和结果</h3>
<p>为了便于测试时编译运行PSRS项目，我编写了<code>CMakelist.txt</code>和<code>build.sh</code>，只需要在终端输入<code>./build.sh</code> 就可以自动编译、链接MPI库。正确运行的结果是在<code>executable</code>的目录下生成一个名为<code>psrs</code>的可执行文件。
然后运行<code>run.py</code>，就可以在data目录下得到全部的实验数据，以csv格式存储。</p>
<h2 id="4-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%90%AB%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90">4. 实验结果（含可视化与分析）</h2>
<p>为了便于可视化调试，我使用了jupyter notebook，利用python的可视化库<code>matplotlib</code>来绘制图表，绘图的python程序在<code>visualize</code>目录下，绘图的结果以png图片格式存在<code>visualize/plots</code>内。</p>
<h3 id="41-%E7%AE%97%E6%B3%95%E7%94%A8%E6%97%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB">4.1 算法用时与数据规模和线程数的关系</h3>
<p>根据运行结果的csv文件，我绘出了这张3D柱状图，它描述了运行时间与线程数量、数组规模的关系：</p>
<p><img src="visualize\plots\PSRS_runtime.png" alt="Alt text"></p>
<p>图中的X，Y轴分别是进程数量、数组大小，纵轴是用时。时间越短，排序速度越快。</p>
<h4 id="411-%E7%AE%97%E6%B3%95%E7%94%A8%E6%97%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1%E7%9A%84%E5%85%B3%E7%B3%BB">4.1.1 算法用时与数据规模的关系</h4>
<p>随着数据规模的增大，排序用时都有不同程度的增加。</p>
<ul>
<li>串行排序用时和数据规模几乎是正比增长的关系。（注意这里array size是指数级增长）</li>
<li>进程数量越多，用时增长随数组规模增大就越慢。进程数为8时，随着数据规模的指数级增长，用时只呈现了线性增长的趋势，体现了并行算法的优势。</li>
</ul>
<h4 id="412-%E7%AE%97%E6%B3%95%E7%94%A8%E6%97%B6%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%95%B0%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB">4.1.2 算法用时与线程数量的关系</h4>
<p>随着进程的增多，排序用时呈现下降趋势。</p>
<ul>
<li>算法用时的减少随着进程数的增加变慢了。2个进程比1个进程几乎快了一倍，但是8个进程比4个进程的提升很有限。</li>
<li>我分析可能是因为通信和同步开销。在并行执行中，不同进程之间需要进行通信和同步，以确保正确的执行顺序和一致性。这些操作引入开销，会成为性能的瓶颈。</li>
</ul>
<h3 id="42-%E5%8A%A0%E9%80%9F%E6%AF%94%E4%B8%8E%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%95%B0%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB">4.2 加速比与数据规模和线程数量的关系</h3>
<p>为了更好的可视化效果，我选择了比较折中的数组规模变化粒度，测量并行排序时间：</p>
<table>
<thead>
<tr>
<th>array size</th>
<th>Processors: 1</th>
<th>Processors: 2</th>
<th>Processors: 4</th>
<th>Processors: 8</th>
</tr>
</thead>
<tbody>
<tr>
<td>(2^{14})</td>
<td>0.001430</td>
<td>0.000788</td>
<td>0.000684</td>
<td>0.000622</td>
</tr>
<tr>
<td>(2^{16})</td>
<td>0.006444</td>
<td>0.003581</td>
<td>0.002201</td>
<td>0.002025</td>
</tr>
<tr>
<td>(2^{18})</td>
<td>0.029598</td>
<td>0.016107</td>
<td>0.010607</td>
<td>0.008047</td>
</tr>
<tr>
<td>(2^{20})</td>
<td>0.129195</td>
<td>0.072732</td>
<td>0.046763</td>
<td>0.031719</td>
</tr>
<tr>
<td>(2^{22})</td>
<td>0.584864</td>
<td>0.327956</td>
<td>0.199234</td>
<td>0.136412</td>
</tr>
</tbody>
</table>
<p>用串行排序时间除以上面的数据，就得到了加速比：</p>
<table>
<thead>
<tr>
<th>(2^n)</th>
<th>Speedup (2)</th>
<th>Speedup (4)</th>
<th>Speedup (8)</th>
</tr>
</thead>
<tbody>
<tr>
<td>(2^{14})</td>
<td>1.814721</td>
<td>2.090643</td>
<td>2.299035</td>
</tr>
<tr>
<td>(2^{16})</td>
<td>1.799497</td>
<td>2.927760</td>
<td>3.182222</td>
</tr>
<tr>
<td>(2^{18})</td>
<td>1.837586</td>
<td>2.790421</td>
<td>3.678141</td>
</tr>
<tr>
<td>(2^{20})</td>
<td>1.776316</td>
<td>2.762761</td>
<td>4.073111</td>
</tr>
<tr>
<td>(2^{22})</td>
<td>1.783361</td>
<td>2.935563</td>
<td>4.287482</td>
</tr>
</tbody>
</table>
<p>这张折线图描述了在不同的数组规模下，加速比与进程数量的关系：</p>
<p><img src="visualize\plots\speedup_vs_procnum_and_size.png" alt="Alt text"></p>
<ul>
<li>总体来看，进程越多，数组规模越大，加速比越高。</li>
<li>随着进程数增加，加速比的增长会放缓。因为随着进程数增多，通信和同步开销会增长。</li>
<li>对于较小的数组规模，进程数设置为4是最好的权衡之选，拥有较高的加速比。如果继续增多进程，加速比增长会变得很不明显。</li>
<li>对于较大的数组规模，进程数可以适当设置得大一些。因为大规模数组可以让并行算法的优势超过它的劣势。</li>
</ul>
<h3 id="43-psrs%E5%90%84%E4%B8%AA%E9%98%B6%E6%AE%B5%E7%9A%84%E7%94%A8%E6%97%B6">4.3 PSRS各个阶段的用时</h3>
<p>下面的簇状柱状图反映了PSRS算法的各个时段的用时。（固定process number = 8）
<img src="visualize\plots\runtime_per_phase.png" alt="Pic"></p>
<ul>
<li>第一阶段（均匀划分、局部排序、正则采样）用时最多，而且增长最快。</li>
<li>第二阶段（选取主元并广播）用时几乎可以忽略。</li>
<li>第三阶段（划分数组、交换数组块）用时随着数组增长的变化不明显。可能是因为通信的开销占主导。</li>
<li>第四阶段（归并排序）用时随着数组大小的增长而增长，且增速与第一阶段差不多。</li>
</ul>
<p>综合下来，局部排序和归并排序的过程的时间开销是最大的。</p>
<h2 id="5-%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%B1%95%E6%9C%9B">5. 结论与展望</h2>
<ul>
<li>并行算法的性能受到多方面因素的影响，包括数据规模、线程数量、任务划分、同步和通信开销等。</li>
<li>MPI提供了灵活的并行编程框架，但需要合理设计算法并注意性能瓶颈，以充分发挥并行计算的优势。</li>
<li>PSRS算法是MPI编程的一个简单应用，未来可以进一步优化算法，尝试其他并行计算框架，如CUDA，以提高排序算法的性能。</li>
</ul>

</body>
</html>
